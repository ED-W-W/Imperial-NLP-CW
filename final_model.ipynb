{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 15:36:05.297066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740324965.319971 2596752 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740324965.327326 2596752 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-23 15:36:05.351838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469\n",
      "9476\n",
      "993\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load data to panda data frame\n",
    "rows=[]\n",
    "with open(os.path.join('dontpatronizeme_pcl.tsv')) as f:\n",
    "    for line in f.readlines()[4:]:\n",
    "        par_id=line.strip().split('\\t')[0]\n",
    "        art_id = line.strip().split('\\t')[1]\n",
    "        keyword=line.strip().split('\\t')[2]\n",
    "        country=line.strip().split('\\t')[3]\n",
    "        t=line.strip().split('\\t')[4]#.lower()\n",
    "        l=line.strip().split('\\t')[-1]\n",
    "        if l=='0' or l=='1':\n",
    "            lbin=0\n",
    "        else:\n",
    "            lbin=1\n",
    "        rows.append(\n",
    "            {'par_id':par_id,\n",
    "            'art_id':art_id,\n",
    "            'keyword':keyword,\n",
    "            'country':country,\n",
    "            'text':t, \n",
    "            'label':lbin, \n",
    "            'orig_label':l\n",
    "            }\n",
    "            )\n",
    "df=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text', 'label', 'orig_label']) \n",
    "print(df.shape[0])\n",
    "print(df[df['label'] == 0].shape[0])\n",
    "print(df[df['label'] == 1].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.87779653862389\n",
      "53.620342396777446\n",
      "48.42248543318369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2596752/4254412181.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noPCL['text_Length'] = noPCL['text'].apply(lambda x: len(x.split()))\n",
      "/tmp/ipykernel_2596752/4254412181.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PCL['text_Length'] = PCL['text'].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter rows with a specific label (e.g., 'negative')\n",
    "noPCL = df[df['label'] == 0]\n",
    "PCL = df[df['label'] == 1]\n",
    "average = df\n",
    "\n",
    "# Get sentence length (in words)\n",
    "noPCL['text_Length'] = noPCL['text'].apply(lambda x: len(x.split()))\n",
    "PCL['text_Length'] = PCL['text'].apply(lambda x: len(x.split()))\n",
    "average['text_length']  = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate average length\n",
    "average_length_noPCL = noPCL['text_Length'].mean()\n",
    "average_length_PCL = PCL['text_Length'].mean()\n",
    "average_l = average['text_length'].mean()\n",
    "\n",
    "print(average_length_noPCL)\n",
    "print(average_length_PCL)\n",
    "print(average_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label    keyword  Count\n",
      "0      0   disabled    947\n",
      "1      0   homeless    899\n",
      "2      0   hopeless    881\n",
      "3      0  immigrant   1031\n",
      "4      0    in-need    906\n",
      "    label        keyword  Count  Percentage\n",
      "0       0       disabled    947    9.993668\n",
      "1       0       homeless    899    9.487125\n",
      "2       0       hopeless    881    9.297172\n",
      "3       0      immigrant   1031   10.880118\n",
      "4       0        in-need    906    9.560996\n",
      "5       0        migrant   1053   11.112284\n",
      "6       0  poor-families    759    8.009709\n",
      "7       0        refugee    982   10.363022\n",
      "8       0     vulnerable   1000   10.552976\n",
      "9       0          women   1018   10.742930\n",
      "10      1       disabled     81    8.157100\n",
      "11      1       homeless    178   17.925478\n",
      "12      1       hopeless    124   12.487412\n",
      "13      1      immigrant     30    3.021148\n",
      "14      1        in-need    176   17.724068\n",
      "15      1        migrant     36    3.625378\n",
      "16      1  poor-families    150   15.105740\n",
      "17      1        refugee     86    8.660624\n",
      "18      1     vulnerable     80    8.056395\n",
      "19      1          women     52    5.236657\n"
     ]
    }
   ],
   "source": [
    "# Group by 'label' and 'keyword', get counts\n",
    "counts = df.groupby(['label', 'keyword']).size().reset_index(name='Count')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(counts.head())\n",
    "\n",
    "# Get total rows per label\n",
    "label_totals = df['label'].value_counts()\n",
    "\n",
    "# Map the total count for each 'label' to the 'Count' DataFrame\n",
    "counts['Label_Total'] = counts['label'].map(label_totals)\n",
    "\n",
    "# Calculate percentage\n",
    "counts['Percentage'] = (counts['Count'] / counts['Label_Total']) * 100\n",
    "\n",
    "# Optional: Drop helper column if you don't need it\n",
    "counts.drop(columns=['Label_Total'], inplace=True)\n",
    "\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label country  Count\n",
      "0      0      au    504\n",
      "1      0      bd    468\n",
      "2      0      ca    484\n",
      "3      0      gb    483\n",
      "4      0      gh    448\n",
      "    label country  Count  Percentage\n",
      "0       0      au    504    5.318700\n",
      "1       0      bd    468    4.938793\n",
      "2       0      ca    484    5.107640\n",
      "3       0      gb    483    5.097087\n",
      "4       0      gh    448    4.727733\n",
      "5       0      hk    461    4.864922\n",
      "6       0      ie    485    5.118193\n",
      "7       0      in    491    5.181511\n",
      "8       0      jm    433    4.569439\n",
      "9       0      ke    494    5.213170\n",
      "10      0      lk    455    4.801604\n",
      "11      0      my    503    5.308147\n",
      "12      0      ng    465    4.907134\n",
      "13      0      nz    471    4.970452\n",
      "14      0      ph    478    5.044322\n",
      "15      0      pk    492    5.192064\n",
      "16      0      sg    497    5.244829\n",
      "17      0      tz    374    3.946813\n",
      "18      0      us    499    5.265935\n",
      "19      0      za    491    5.181511\n",
      "20      1      au     37    3.726083\n",
      "21      1      bd     44    4.431017\n",
      "22      1      ca     46    4.632427\n",
      "23      1      gb     57    5.740181\n",
      "24      1      gh     75    7.552870\n",
      "25      1      hk     29    2.920443\n",
      "26      1      ie     52    5.236657\n",
      "27      1      in     39    3.927492\n",
      "28      1      jm     57    5.740181\n",
      "29      1      ke     45    4.531722\n",
      "30      1      lk     49    4.934542\n",
      "31      1      my     43    4.330312\n",
      "32      1      ng     72    7.250755\n",
      "33      1      nz     47    4.733132\n",
      "34      1      ph     67    6.747231\n",
      "35      1      pk     53    5.337362\n",
      "36      1      sg     38    3.826788\n",
      "37      1      tz     41    4.128902\n",
      "38      1      us     44    4.431017\n",
      "39      1      za     58    5.840886\n"
     ]
    }
   ],
   "source": [
    "# Group by 'label' and 'keyword', get counts\n",
    "counts = df.groupby(['label', 'country']).size().reset_index(name='Count')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(counts.head())\n",
    "\n",
    "# Get total rows per label\n",
    "label_totals = df['label'].value_counts()\n",
    "\n",
    "# Map the total count for each 'label' to the 'Count' DataFrame\n",
    "counts['Label_Total'] = counts['label'].map(label_totals)\n",
    "\n",
    "# Calculate percentage\n",
    "counts['Percentage'] = (counts['Count'] / counts['Label_Total']) * 100\n",
    "\n",
    "# Optional: Drop helper column if you don't need it\n",
    "counts.drop(columns=['Label_Total'], inplace=True)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469\n"
     ]
    }
   ],
   "source": [
    "# Split in to dev set and test set\n",
    "train_ids = pd.read_csv('train_semeval_parids-labels.csv')\n",
    "dev_ids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "train_ids = train_ids.iloc[:, 0].astype(str).tolist()\n",
    "dev_ids = dev_ids.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "train_df = df[df['par_id'].isin(train_ids)]\n",
    "dev_df = df[df['par_id'].isin(dev_ids)]\n",
    "print(len(train_df)+len(dev_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = df['keyword'].unique().tolist()\n",
    "COUNTRIES = df['country'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass PCLDataset(Dataset):\\n    def __init__(self, texts, labels, keywords, countries, tokenizer, max_length=128, shuffle=True):\\n        if shuffle:\\n            combined = list(zip(texts, labels, keywords, countries))\\n            random.shuffle(combined)\\n            texts, labels, keywords, countries = zip(*combined)\\n            texts, labels, keywords, countries = list(texts), list(labels), list(keywords), list(countries)\\n        \\n        # Tokenize texts\\n        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\\n        self.labels = labels\\n        # Compute extra features for each text (e.g. sentence length and keyword indicator)\\n        #self.extra_features = [self.compute_extra_features(text) for text in texts]\\n        #self.extra_features = [self.extract_features(text, keyword, country) for text, keyword, country in zip(texts, keywords, countries)]\\n        \\n    def extract_features(self, text, keyword, country):\\n        length = len(text.split())\\n\\n        # One-hot keyword presence\\n        keyword_presence = [1 if keyword == kw else 0 for kw in KEYWORDS]\\n\\n        # One-hot encoding of country\\n        country_encoding = [1 if country == c else 0 for c in COUNTRIES]\\n\\n        # Combine all features\\n        features = [length] + keyword_presence + country_encoding\\n        return features\\n    \\n    def __getitem__(self, idx):\\n        # Get tokenized text\\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\n        # Get label\\n        item['labels'] = torch.tensor(self.labels[idx])\\n        # Get extra features (convert to float tensor)\\n        extra_feature = torch.tensor(self.extra_features[idx], dtype=torch.float)\\n        item['extra_features'] = extra_feature\\n        return item\\n\\n    def __len__(self):\\n        return len(self.labels)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, labels, keywords, countries, tokenizer, max_length=128, shuffle=True):\n",
    "        if shuffle:\n",
    "            combined = list(zip(texts, labels, keywords, countries))\n",
    "            random.shuffle(combined)\n",
    "            texts, labels, keywords, countries = zip(*combined)\n",
    "            texts, labels, keywords, countries = list(texts), list(labels), list(keywords), list(countries)\n",
    "        \n",
    "        # Tokenize texts\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "        # Compute extra features for each text (e.g. sentence length and keyword indicator)\n",
    "        #self.extra_features = [self.compute_extra_features(text) for text in texts]\n",
    "        #self.extra_features = [self.extract_features(text, keyword, country) for text, keyword, country in zip(texts, keywords, countries)]\n",
    "        \n",
    "    def extract_features(self, text, keyword, country):\n",
    "        length = len(text.split())\n",
    "\n",
    "        # One-hot keyword presence\n",
    "        keyword_presence = [1 if keyword == kw else 0 for kw in KEYWORDS]\n",
    "\n",
    "        # One-hot encoding of country\n",
    "        country_encoding = [1 if country == c else 0 for c in COUNTRIES]\n",
    "\n",
    "        # Combine all features\n",
    "        features = [length] + keyword_presence + country_encoding\n",
    "        return features\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get tokenized text\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Get label\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        # Get extra features (convert to float tensor)\n",
    "        extra_feature = torch.tensor(self.extra_features[idx], dtype=torch.float)\n",
    "        item['extra_features'] = extra_feature\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass PCLModel(nn.Module):\\n    def __init__(self, model_name, num_labels, extra_feature_dim):\\n        super(PCLModel, self).__init__()\\n        self.model = AutoModel.from_pretrained(model_name)\\n        hidden_size = self.model.config.hidden_size  # typically 768 for roberta-base\\n        self.dropout = nn.Dropout(0.1)\\n        # Classifier takes concatenated [CLS] embedding and extra features\\n        #self.classifier = nn.Linear(hidden_size + extra_feature_dim, num_labels)\\n        self.classifier = nn.Sequential(\\n            nn.Linear(hidden_size + extra_feature_dim, 256),  \\n            nn.ReLU(),\\n            nn.Dropout(0.1),\\n            nn.Linear(256, num_labels)\\n        )\\n\\n    def forward(self, input_ids, attention_mask, extra_features, labels=None):\\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\\n        cls_output = outputs.pooler_output  # [CLS] token representation\\n        # Concatenate the extra features\\n        combined = torch.cat((cls_output, extra_features), dim=1)\\n        combined = self.dropout(combined)\\n        logits = self.classifier(combined)\\n        loss = None\\n        if labels is not None:\\n            loss_fct = nn.CrossEntropyLoss()\\n            loss = loss_fct(logits, labels)\\n        return {\"loss\": loss, \"logits\": logits}\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class PCLModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, extra_feature_dim):\n",
    "        super(PCLModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size  # typically 768 for roberta-base\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Classifier takes concatenated [CLS] embedding and extra features\n",
    "        #self.classifier = nn.Linear(hidden_size + extra_feature_dim, num_labels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size + extra_feature_dim, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, extra_features, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output  # [CLS] token representation\n",
    "        # Concatenate the extra features\n",
    "        combined = torch.cat((cls_output, extra_features), dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.classifier(combined)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, shuffle=True):\n",
    "        # Tokenize texts\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "        # Compute extra features for each text (e.g. sentence length and keyword indicator)\n",
    "        #self.extra_features = [self.compute_extra_features(text) for text in texts]\n",
    "        #self.extra_features = [self.extract_features(text, keyword, country) for text, keyword, country in zip(texts, keywords, countries)]\n",
    "        \n",
    "    def extract_features(self, text, keyword, country):\n",
    "        length = len(text.split())\n",
    "\n",
    "        # One-hot keyword presence\n",
    "        keyword_presence = [1 if keyword == kw else 0 for kw in KEYWORDS]\n",
    "\n",
    "        # One-hot encoding of country\n",
    "        country_encoding = [1 if country == c else 0 for c in COUNTRIES]\n",
    "\n",
    "        # Combine all features\n",
    "        features = [length] + keyword_presence + country_encoding\n",
    "        return features\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get tokenized text\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Get label\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        # Get extra features (convert to float tensor)\n",
    "        #extra_feature = torch.tensor(self.extra_features[idx], dtype=torch.float)\n",
    "        #item['extra_features'] = extra_feature\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "class PCLModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(PCLModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size  # typically 768 for roberta-base\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Classifier takes concatenated [CLS] embedding and extra features\n",
    "        #self.classifier = nn.Linear(hidden_size + extra_feature_dim, num_labels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #cls_output = outputs.pooler_output  # [CLS] token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # Take the [CLS] token representation\n",
    "        # Concatenate the extra features\n",
    "        combined = cls_output\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.classifier(combined)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "#model_name = \"roberta-base\"  # You can switch to \"roberta-large\" if desired\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Load the tokenizer (if not already loaded)\n",
    "#model_name = \"roberta-base\" # cased\n",
    "model_name = \"microsoft/deberta-base\"  # DeBERTa-base (cased)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the custom model\n",
    "num_labels = 2  # binary classification\n",
    "extra_feature_dim = 1 + len(KEYWORDS) + len(COUNTRIES)\n",
    "#model = PCLModel(model_name, num_labels, extra_feature_dim)\n",
    "model = PCLModel(model_name, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_texts = train_df[\"text\"].tolist()\n",
    "#train_labels = train_df[\"label\"].tolist()\n",
    "\n",
    "# Split train set into train and valiadation (90/10)\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "#    train_texts, train_labels, test_size=0.1, random_state=42\n",
    "#)\n",
    "\n",
    "# Create dataset objects for training and validation\n",
    "#train_dataset = PCLDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "#val_dataset = PCLDataset(val_texts, val_labels, tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 13891, Validation samples: 838\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the original data into training and validation sets *before* augmentation\n",
    "train_texts, val_texts, train_labels, val_labels, train_keywords, val_keywords, train_countries, val_countries = train_test_split(\n",
    "    train_df[\"text\"].tolist(),\n",
    "    train_df[\"label\"].tolist(),\n",
    "    train_df[\"keyword\"].tolist(),\n",
    "    train_df[\"country\"].tolist(),\n",
    "    test_size=0.1,  # 20% for validation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Separate majority and minority classes (assuming label 1 is minority)\n",
    "minority_indices = [i for i, label in enumerate(train_labels) if label == 1]\n",
    "minority_texts = [train_texts[i] for i in minority_indices]\n",
    "minority_keywords = [train_keywords[i] for i in minority_indices]\n",
    "minority_countries = [train_countries[i] for i in minority_indices]\n",
    "\n",
    "# Data augmentation with Contextual BERT substitution\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased',\n",
    "    action=\"substitute\",  # or \"insert\"\n",
    "    device='cuda'  # or 'cpu'\n",
    ")\n",
    "\n",
    "augmented_texts = []\n",
    "augmented_keywords = []\n",
    "augmented_countries = []\n",
    "num_aug = 9\n",
    "for text, keyword, country in zip(minority_texts, minority_keywords, minority_countries):\n",
    "    # Generate 9 augmentations per minority sample\n",
    "    augmented_texts.extend([aug.augment(text) for _ in range(num_aug)])\n",
    "    augmented_keywords.extend([keyword] * num_aug)\n",
    "    augmented_countries.extend([country] * num_aug)\n",
    "\n",
    "augmented_labels = [1] * len(augmented_texts)\n",
    "\n",
    "# Combine augmented minority samples with the original training data (not validation)\n",
    "train_texts += augmented_texts\n",
    "train_labels += augmented_labels\n",
    "train_keywords += augmented_keywords\n",
    "train_countries += augmented_countries\n",
    "\n",
    "# Ensure train_texts is a flat list (if augmentation returns nested lists)\n",
    "train_texts = [text if isinstance(text, str) else text[0] for text in train_texts]\n",
    "\n",
    "# Create dataset objects\n",
    "#train_dataset = PCLDataset(train_texts, train_labels, train_keywords, train_countries, tokenizer, max_length=128)\n",
    "#val_dataset = PCLDataset(val_texts, val_labels, val_keywords, val_countries, tokenizer, max_length=128)\n",
    "train_dataset = PCLDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = PCLDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after augmentation:\n",
      "Class 0: 6831 instances\n",
      "Class 1: 7060 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution after augmentation:\")\n",
    "print(f\"Class 0: {train_labels.count(0)} instances\")\n",
    "print(f\"Class 1: {train_labels.count(1)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/eww24/nlpenv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # Output directory for model checkpoints\n",
    "    num_train_epochs=3,                # Number of training epochs\n",
    "    per_device_train_batch_size=16,    # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,     # Batch size for evaluation\n",
    "    learning_rate=2e-5,                # Initial learning rate\n",
    "    warmup_steps=500,                  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                 # Strength of weight decay\n",
    "    logging_dir='./logs',              # Directory for storing logs\n",
    "    logging_steps=10,                  # Log every 10 steps\n",
    "    evaluation_strategy=\"steps\",       # Evaluate every 'eval_steps'\n",
    "    eval_steps=100,                    # Evaluate every 100 steps\n",
    "    save_steps=100,                    # Save checkpoint every 100 steps\n",
    "    load_best_model_at_end=True,       # Load the best model at the end\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,           # Lower eval_loss is better\n",
    "    disable_tqdm=False,\n",
    "    report_to=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='2607' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/2607 14:10 < 06:21, 2.11 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.484700</td>\n",
       "      <td>0.769597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.391098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.311611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.325564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.381393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.260660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.298632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.232367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.229247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.247956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.238692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.217967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.336475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.218951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.216042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.257449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.229342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.284218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1800, training_loss=0.1751545145859321, metrics={'train_runtime': 851.0095, 'train_samples_per_second': 48.969, 'train_steps_per_second': 3.063, 'total_flos': 0.0, 'train_loss': 0.1751545145859321, 'epoch': 2.0713463751438437})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# Initialize and start the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if no improvement for 3 evaluations\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/53 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.21604157984256744, 'eval_runtime': 5.6897, 'eval_samples_per_second': 147.285, 'eval_steps_per_second': 9.315, 'epoch': 2.0713463751438437}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_pcl_model\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "save_directory = \"./fine_tuned_pcl_model\"\n",
    "trainer.save_model(save_directory)  # This saves the model to the specified directory\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./fine_tuned_pcl_model were not used when initializing RobertaForSequenceClassification: ['classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.3.weight', 'model.embeddings.LayerNorm.bias', 'model.embeddings.LayerNorm.weight', 'model.embeddings.word_embeddings.weight', 'model.encoder.layer.0.attention.output.LayerNorm.bias', 'model.encoder.layer.0.attention.output.LayerNorm.weight', 'model.encoder.layer.0.attention.output.dense.bias', 'model.encoder.layer.0.attention.output.dense.weight', 'model.encoder.layer.0.attention.self.in_proj.weight', 'model.encoder.layer.0.attention.self.pos_proj.weight', 'model.encoder.layer.0.attention.self.pos_q_proj.bias', 'model.encoder.layer.0.attention.self.pos_q_proj.weight', 'model.encoder.layer.0.attention.self.q_bias', 'model.encoder.layer.0.attention.self.v_bias', 'model.encoder.layer.0.intermediate.dense.bias', 'model.encoder.layer.0.intermediate.dense.weight', 'model.encoder.layer.0.output.LayerNorm.bias', 'model.encoder.layer.0.output.LayerNorm.weight', 'model.encoder.layer.0.output.dense.bias', 'model.encoder.layer.0.output.dense.weight', 'model.encoder.layer.1.attention.output.LayerNorm.bias', 'model.encoder.layer.1.attention.output.LayerNorm.weight', 'model.encoder.layer.1.attention.output.dense.bias', 'model.encoder.layer.1.attention.output.dense.weight', 'model.encoder.layer.1.attention.self.in_proj.weight', 'model.encoder.layer.1.attention.self.pos_proj.weight', 'model.encoder.layer.1.attention.self.pos_q_proj.bias', 'model.encoder.layer.1.attention.self.pos_q_proj.weight', 'model.encoder.layer.1.attention.self.q_bias', 'model.encoder.layer.1.attention.self.v_bias', 'model.encoder.layer.1.intermediate.dense.bias', 'model.encoder.layer.1.intermediate.dense.weight', 'model.encoder.layer.1.output.LayerNorm.bias', 'model.encoder.layer.1.output.LayerNorm.weight', 'model.encoder.layer.1.output.dense.bias', 'model.encoder.layer.1.output.dense.weight', 'model.encoder.layer.10.attention.output.LayerNorm.bias', 'model.encoder.layer.10.attention.output.LayerNorm.weight', 'model.encoder.layer.10.attention.output.dense.bias', 'model.encoder.layer.10.attention.output.dense.weight', 'model.encoder.layer.10.attention.self.in_proj.weight', 'model.encoder.layer.10.attention.self.pos_proj.weight', 'model.encoder.layer.10.attention.self.pos_q_proj.bias', 'model.encoder.layer.10.attention.self.pos_q_proj.weight', 'model.encoder.layer.10.attention.self.q_bias', 'model.encoder.layer.10.attention.self.v_bias', 'model.encoder.layer.10.intermediate.dense.bias', 'model.encoder.layer.10.intermediate.dense.weight', 'model.encoder.layer.10.output.LayerNorm.bias', 'model.encoder.layer.10.output.LayerNorm.weight', 'model.encoder.layer.10.output.dense.bias', 'model.encoder.layer.10.output.dense.weight', 'model.encoder.layer.11.attention.output.LayerNorm.bias', 'model.encoder.layer.11.attention.output.LayerNorm.weight', 'model.encoder.layer.11.attention.output.dense.bias', 'model.encoder.layer.11.attention.output.dense.weight', 'model.encoder.layer.11.attention.self.in_proj.weight', 'model.encoder.layer.11.attention.self.pos_proj.weight', 'model.encoder.layer.11.attention.self.pos_q_proj.bias', 'model.encoder.layer.11.attention.self.pos_q_proj.weight', 'model.encoder.layer.11.attention.self.q_bias', 'model.encoder.layer.11.attention.self.v_bias', 'model.encoder.layer.11.intermediate.dense.bias', 'model.encoder.layer.11.intermediate.dense.weight', 'model.encoder.layer.11.output.LayerNorm.bias', 'model.encoder.layer.11.output.LayerNorm.weight', 'model.encoder.layer.11.output.dense.bias', 'model.encoder.layer.11.output.dense.weight', 'model.encoder.layer.2.attention.output.LayerNorm.bias', 'model.encoder.layer.2.attention.output.LayerNorm.weight', 'model.encoder.layer.2.attention.output.dense.bias', 'model.encoder.layer.2.attention.output.dense.weight', 'model.encoder.layer.2.attention.self.in_proj.weight', 'model.encoder.layer.2.attention.self.pos_proj.weight', 'model.encoder.layer.2.attention.self.pos_q_proj.bias', 'model.encoder.layer.2.attention.self.pos_q_proj.weight', 'model.encoder.layer.2.attention.self.q_bias', 'model.encoder.layer.2.attention.self.v_bias', 'model.encoder.layer.2.intermediate.dense.bias', 'model.encoder.layer.2.intermediate.dense.weight', 'model.encoder.layer.2.output.LayerNorm.bias', 'model.encoder.layer.2.output.LayerNorm.weight', 'model.encoder.layer.2.output.dense.bias', 'model.encoder.layer.2.output.dense.weight', 'model.encoder.layer.3.attention.output.LayerNorm.bias', 'model.encoder.layer.3.attention.output.LayerNorm.weight', 'model.encoder.layer.3.attention.output.dense.bias', 'model.encoder.layer.3.attention.output.dense.weight', 'model.encoder.layer.3.attention.self.in_proj.weight', 'model.encoder.layer.3.attention.self.pos_proj.weight', 'model.encoder.layer.3.attention.self.pos_q_proj.bias', 'model.encoder.layer.3.attention.self.pos_q_proj.weight', 'model.encoder.layer.3.attention.self.q_bias', 'model.encoder.layer.3.attention.self.v_bias', 'model.encoder.layer.3.intermediate.dense.bias', 'model.encoder.layer.3.intermediate.dense.weight', 'model.encoder.layer.3.output.LayerNorm.bias', 'model.encoder.layer.3.output.LayerNorm.weight', 'model.encoder.layer.3.output.dense.bias', 'model.encoder.layer.3.output.dense.weight', 'model.encoder.layer.4.attention.output.LayerNorm.bias', 'model.encoder.layer.4.attention.output.LayerNorm.weight', 'model.encoder.layer.4.attention.output.dense.bias', 'model.encoder.layer.4.attention.output.dense.weight', 'model.encoder.layer.4.attention.self.in_proj.weight', 'model.encoder.layer.4.attention.self.pos_proj.weight', 'model.encoder.layer.4.attention.self.pos_q_proj.bias', 'model.encoder.layer.4.attention.self.pos_q_proj.weight', 'model.encoder.layer.4.attention.self.q_bias', 'model.encoder.layer.4.attention.self.v_bias', 'model.encoder.layer.4.intermediate.dense.bias', 'model.encoder.layer.4.intermediate.dense.weight', 'model.encoder.layer.4.output.LayerNorm.bias', 'model.encoder.layer.4.output.LayerNorm.weight', 'model.encoder.layer.4.output.dense.bias', 'model.encoder.layer.4.output.dense.weight', 'model.encoder.layer.5.attention.output.LayerNorm.bias', 'model.encoder.layer.5.attention.output.LayerNorm.weight', 'model.encoder.layer.5.attention.output.dense.bias', 'model.encoder.layer.5.attention.output.dense.weight', 'model.encoder.layer.5.attention.self.in_proj.weight', 'model.encoder.layer.5.attention.self.pos_proj.weight', 'model.encoder.layer.5.attention.self.pos_q_proj.bias', 'model.encoder.layer.5.attention.self.pos_q_proj.weight', 'model.encoder.layer.5.attention.self.q_bias', 'model.encoder.layer.5.attention.self.v_bias', 'model.encoder.layer.5.intermediate.dense.bias', 'model.encoder.layer.5.intermediate.dense.weight', 'model.encoder.layer.5.output.LayerNorm.bias', 'model.encoder.layer.5.output.LayerNorm.weight', 'model.encoder.layer.5.output.dense.bias', 'model.encoder.layer.5.output.dense.weight', 'model.encoder.layer.6.attention.output.LayerNorm.bias', 'model.encoder.layer.6.attention.output.LayerNorm.weight', 'model.encoder.layer.6.attention.output.dense.bias', 'model.encoder.layer.6.attention.output.dense.weight', 'model.encoder.layer.6.attention.self.in_proj.weight', 'model.encoder.layer.6.attention.self.pos_proj.weight', 'model.encoder.layer.6.attention.self.pos_q_proj.bias', 'model.encoder.layer.6.attention.self.pos_q_proj.weight', 'model.encoder.layer.6.attention.self.q_bias', 'model.encoder.layer.6.attention.self.v_bias', 'model.encoder.layer.6.intermediate.dense.bias', 'model.encoder.layer.6.intermediate.dense.weight', 'model.encoder.layer.6.output.LayerNorm.bias', 'model.encoder.layer.6.output.LayerNorm.weight', 'model.encoder.layer.6.output.dense.bias', 'model.encoder.layer.6.output.dense.weight', 'model.encoder.layer.7.attention.output.LayerNorm.bias', 'model.encoder.layer.7.attention.output.LayerNorm.weight', 'model.encoder.layer.7.attention.output.dense.bias', 'model.encoder.layer.7.attention.output.dense.weight', 'model.encoder.layer.7.attention.self.in_proj.weight', 'model.encoder.layer.7.attention.self.pos_proj.weight', 'model.encoder.layer.7.attention.self.pos_q_proj.bias', 'model.encoder.layer.7.attention.self.pos_q_proj.weight', 'model.encoder.layer.7.attention.self.q_bias', 'model.encoder.layer.7.attention.self.v_bias', 'model.encoder.layer.7.intermediate.dense.bias', 'model.encoder.layer.7.intermediate.dense.weight', 'model.encoder.layer.7.output.LayerNorm.bias', 'model.encoder.layer.7.output.LayerNorm.weight', 'model.encoder.layer.7.output.dense.bias', 'model.encoder.layer.7.output.dense.weight', 'model.encoder.layer.8.attention.output.LayerNorm.bias', 'model.encoder.layer.8.attention.output.LayerNorm.weight', 'model.encoder.layer.8.attention.output.dense.bias', 'model.encoder.layer.8.attention.output.dense.weight', 'model.encoder.layer.8.attention.self.in_proj.weight', 'model.encoder.layer.8.attention.self.pos_proj.weight', 'model.encoder.layer.8.attention.self.pos_q_proj.bias', 'model.encoder.layer.8.attention.self.pos_q_proj.weight', 'model.encoder.layer.8.attention.self.q_bias', 'model.encoder.layer.8.attention.self.v_bias', 'model.encoder.layer.8.intermediate.dense.bias', 'model.encoder.layer.8.intermediate.dense.weight', 'model.encoder.layer.8.output.LayerNorm.bias', 'model.encoder.layer.8.output.LayerNorm.weight', 'model.encoder.layer.8.output.dense.bias', 'model.encoder.layer.8.output.dense.weight', 'model.encoder.layer.9.attention.output.LayerNorm.bias', 'model.encoder.layer.9.attention.output.LayerNorm.weight', 'model.encoder.layer.9.attention.output.dense.bias', 'model.encoder.layer.9.attention.output.dense.weight', 'model.encoder.layer.9.attention.self.in_proj.weight', 'model.encoder.layer.9.attention.self.pos_proj.weight', 'model.encoder.layer.9.attention.self.pos_q_proj.bias', 'model.encoder.layer.9.attention.self.pos_q_proj.weight', 'model.encoder.layer.9.attention.self.q_bias', 'model.encoder.layer.9.attention.self.v_bias', 'model.encoder.layer.9.intermediate.dense.bias', 'model.encoder.layer.9.intermediate.dense.weight', 'model.encoder.layer.9.output.LayerNorm.bias', 'model.encoder.layer.9.output.LayerNorm.weight', 'model.encoder.layer.9.output.dense.bias', 'model.encoder.layer.9.output.dense.weight', 'model.encoder.rel_embeddings.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./fine_tuned_pcl_model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "\n",
    "#loaded_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
    "#loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_texts = dev_df[\"text\"].tolist()\n",
    "dev_labels = dev_df[\"label\"].tolist()\n",
    "dev_keywords = dev_df[\"keyword\"].tolist()\n",
    "dev_countries = dev_df[\"country\"].tolist()\n",
    "\n",
    "\n",
    "#dev_dataset = PCLDataset(dev_texts, dev_labels, dev_keywords, dev_countries, tokenizer, max_length=128)\n",
    "dev_dataset = PCLDataset(dev_texts, dev_labels, tokenizer, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Accuracy: 0.9202483285577842\n",
      "Dev F1: 0.5322128851540616\n",
      "Dev Precision: 0.6012658227848101\n",
      "Dev Recall: 0.47738693467336685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Get predictions using the trainer\n",
    "pred_output = trainer.predict(dev_dataset)\n",
    "pred_labels = np.argmax(pred_output.predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(dev_labels, pred_labels)\n",
    "f1 = f1_score(dev_labels, pred_labels, average='binary')\n",
    "precision = precision_score(dev_labels, pred_labels, average='binary')\n",
    "recall = recall_score(dev_labels, pred_labels, average='binary')\n",
    "\n",
    "print(\"Dev Accuracy:\", accuracy)\n",
    "print(\"Dev F1:\", f1)\n",
    "print(\"Dev Precision:\", precision)\n",
    "print(\"Dev Recall:\", recall)\n",
    "\n",
    "# Write predicted class (0 or 1) to dev.txt, one prediction per line\n",
    "with open(\"dev.txt\", \"w\") as f:\n",
    "    for pred in pred_labels:\n",
    "        f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832\n"
     ]
    }
   ],
   "source": [
    "# Load the final test set\n",
    "rows=[]\n",
    "with open(os.path.join('task4_test.tsv')) as f:\n",
    "    for line in f.readlines():\n",
    "        par_id=line.strip().split('\\t')[0]\n",
    "        art_id = line.strip().split('\\t')[1]\n",
    "        keyword=line.strip().split('\\t')[2]\n",
    "        country=line.strip().split('\\t')[3]\n",
    "        t=line.strip().split('\\t')[4]#.lower()\n",
    "        rows.append(\n",
    "            {'par_id':par_id,\n",
    "            'art_id':art_id,\n",
    "            'keyword':keyword,\n",
    "            'country':country,\n",
    "            'text':t, \n",
    "            }\n",
    "            )\n",
    "test_df=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text']) \n",
    "print(test_df.shape[0])\n",
    "\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "test_labels = [0] * len(test_texts)\n",
    "test_keywords = test_df[\"keyword\"].tolist()\n",
    "test_countries = test_df[\"country\"].tolist()\n",
    "\n",
    "#test_dataset = PCLDataset(test_texts, test_labels, test_keywords, test_countries, tokenizer, max_length=128)\n",
    "test_dataset = PCLDataset(test_texts, test_labels, tokenizer, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832\n"
     ]
    }
   ],
   "source": [
    "pred_output = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(pred_output.predictions, axis=1)\n",
    "\n",
    "print(len(pred_labels))\n",
    "\n",
    "# Write predicted class (0 or 1) to test.txt, one prediction per line\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    for pred in pred_labels:\n",
    "        f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Use the loaded model to make predictions\\nfrom sklearn.metrics import f1_score\\n\\nloaded_model.eval()\\ncorrect = 0\\npredictions = []\\nfor i in range(len(dev_texts)):\\n    text = dev_texts[i]\\n    encodings = loaded_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\\n    loaded_model.eval()\\n    with torch.no_grad():\\n        outputs = loaded_model(**encodings)\\n        pred_label = torch.argmax(outputs.logits, dim=1)\\n        if pred_label.tolist()[0] == dev_labels[i]:\\n            correct += 1\\n        #print(\"Predictions:\", predictions.tolist()[0], \"Actual label:\", dev_labels[i])\\n        predictions.append(pred_label.tolist()[0])\\nprint(correct)\\naccuracy = correct/len(dev_texts)\\nf1 = f1_score(dev_labels, predictions, average=\\'binary\\')\\nprint(\"Accuracy:\", accuracy, \"F1:\", f1)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Use the loaded model to make predictions\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "loaded_model.eval()\n",
    "correct = 0\n",
    "predictions = []\n",
    "for i in range(len(dev_texts)):\n",
    "    text = dev_texts[i]\n",
    "    encodings = loaded_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    loaded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**encodings)\n",
    "        pred_label = torch.argmax(outputs.logits, dim=1)\n",
    "        if pred_label.tolist()[0] == dev_labels[i]:\n",
    "            correct += 1\n",
    "        #print(\"Predictions:\", predictions.tolist()[0], \"Actual label:\", dev_labels[i])\n",
    "        predictions.append(pred_label.tolist()[0])\n",
    "print(correct)\n",
    "accuracy = correct/len(dev_texts)\n",
    "f1 = f1_score(dev_labels, predictions, average='binary')\n",
    "print(\"Accuracy:\", accuracy, \"F1:\", f1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.897803247373448\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95      1895\n",
      "           1       0.41      0.18      0.25       199\n",
      "\n",
      "    accuracy                           0.90      2094\n",
      "   macro avg       0.67      0.58      0.60      2094\n",
      "weighted avg       0.87      0.90      0.88      2094\n",
      "\n",
      "\n",
      "Top features for PCL detection:\n",
      "christmas: 2.1195\n",
      "dreamers: 1.8463\n",
      "hungry: 1.7233\n",
      "hope: 1.6972\n",
      "dignity: 1.5203\n",
      "donate: 1.4944\n",
      "duty: 1.4201\n",
      "delivered: 1.4135\n",
      "underprivileged: 1.4076\n",
      "lives: 1.3839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df['text'], train_df['label'], test_size=0.3, random_state=42, stratify=train_df['label']\n",
    ")\n",
    "\n",
    "# Initialize the CountVectorizer (Bag-of-Words)\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_bow = vectorizer.fit_transform(train_df['text'])\n",
    "X_test_bow = vectorizer.transform(dev_df['text'])\n",
    "\n",
    "# Initialize and train a simple classifier. Here we use Logistic Regression.\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(X_train_bow, train_df['label'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(dev_df['label'], y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(dev_df['label'], y_pred))\n",
    "\n",
    "# Optionally: inspect the top features associated with PCL detection.\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = clf.coef_[0]\n",
    "# Get the indices of the top 10 features for PCL (i.e., with highest positive coefficients)\n",
    "top_positive_indices = coef.argsort()[-10:][::-1]\n",
    "print(\"\\nTop features for PCL detection:\")\n",
    "for idx in top_positive_indices:\n",
    "    print(f\"{feature_names[idx]}: {coef[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9059216809933143\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      1895\n",
      "           1       0.60      0.03      0.06       199\n",
      "\n",
      "    accuracy                           0.91      2094\n",
      "   macro avg       0.75      0.51      0.50      2094\n",
      "weighted avg       0.88      0.91      0.87      2094\n",
      "\n",
      "\n",
      "Top features for PCL detection:\n",
      "hope: 3.6782\n",
      "lives: 3.1356\n",
      "help: 3.1244\n",
      "homeless: 3.1015\n",
      "need: 3.0978\n",
      "life: 3.0676\n",
      "poor: 2.8790\n",
      "christmas: 2.8656\n",
      "children: 2.5291\n",
      "poverty: 2.4010\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.3, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(dev_df['text'])\n",
    "\n",
    "# Initialize and train a logistic regression classifier.\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(dev_df['label'], y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(dev_df['label'], y_pred))\n",
    "\n",
    "# Optionally: inspect the top features associated with PCL detection.\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "coef = clf.coef_[0]\n",
    "# Get the indices of the top 10 features for PCL (i.e., with highest positive coefficients)\n",
    "top_positive_indices = coef.argsort()[-10:][::-1]\n",
    "print(\"\\nTop features for PCL detection:\")\n",
    "for idx in top_positive_indices:\n",
    "    print(f\"{feature_names[idx]}: {coef[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
