{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 21:25:28.299219: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-19 21:25:28.310866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740000328.324066 3059259 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740000328.327991 3059259 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 21:25:28.343132: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469\n",
      "9476\n",
      "993\n"
     ]
    }
   ],
   "source": [
    "# Load data to panda data frame\n",
    "rows=[]\n",
    "with open(os.path.join('dontpatronizeme_pcl.tsv')) as f:\n",
    "    for line in f.readlines()[4:]:\n",
    "        par_id=line.strip().split('\\t')[0]\n",
    "        art_id = line.strip().split('\\t')[1]\n",
    "        keyword=line.strip().split('\\t')[2]\n",
    "        country=line.strip().split('\\t')[3]\n",
    "        t=line.strip().split('\\t')[4]#.lower()\n",
    "        l=line.strip().split('\\t')[-1]\n",
    "        if l=='0' or l=='1':\n",
    "            lbin=0\n",
    "        else:\n",
    "            lbin=1\n",
    "        rows.append(\n",
    "            {'par_id':par_id,\n",
    "            'art_id':art_id,\n",
    "            'keyword':keyword,\n",
    "            'country':country,\n",
    "            'text':t, \n",
    "            'label':lbin, \n",
    "            'orig_label':l\n",
    "            }\n",
    "            )\n",
    "df=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text', 'label', 'orig_label']) \n",
    "print(df.shape[0])\n",
    "print(df[df['label'] == 0].shape[0])\n",
    "print(df[df['label'] == 1].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.87779653862389\n",
      "53.620342396777446\n",
      "48.42248543318369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3059259/4254412181.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noPCL['text_Length'] = noPCL['text'].apply(lambda x: len(x.split()))\n",
      "/tmp/ipykernel_3059259/4254412181.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PCL['text_Length'] = PCL['text'].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter rows with a specific label (e.g., 'negative')\n",
    "noPCL = df[df['label'] == 0]\n",
    "PCL = df[df['label'] == 1]\n",
    "average = df\n",
    "\n",
    "# Get sentence length (in words)\n",
    "noPCL['text_Length'] = noPCL['text'].apply(lambda x: len(x.split()))\n",
    "PCL['text_Length'] = PCL['text'].apply(lambda x: len(x.split()))\n",
    "average['text_length']  = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate average length\n",
    "average_length_noPCL = noPCL['text_Length'].mean()\n",
    "average_length_PCL = PCL['text_Length'].mean()\n",
    "average_l = average['text_length'].mean()\n",
    "\n",
    "print(average_length_noPCL)\n",
    "print(average_length_PCL)\n",
    "print(average_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label    keyword  Count\n",
      "0      0   disabled    947\n",
      "1      0   homeless    899\n",
      "2      0   hopeless    881\n",
      "3      0  immigrant   1031\n",
      "4      0    in-need    906\n",
      "    label        keyword  Count  Percentage\n",
      "0       0       disabled    947    9.993668\n",
      "1       0       homeless    899    9.487125\n",
      "2       0       hopeless    881    9.297172\n",
      "3       0      immigrant   1031   10.880118\n",
      "4       0        in-need    906    9.560996\n",
      "5       0        migrant   1053   11.112284\n",
      "6       0  poor-families    759    8.009709\n",
      "7       0        refugee    982   10.363022\n",
      "8       0     vulnerable   1000   10.552976\n",
      "9       0          women   1018   10.742930\n",
      "10      1       disabled     81    8.157100\n",
      "11      1       homeless    178   17.925478\n",
      "12      1       hopeless    124   12.487412\n",
      "13      1      immigrant     30    3.021148\n",
      "14      1        in-need    176   17.724068\n",
      "15      1        migrant     36    3.625378\n",
      "16      1  poor-families    150   15.105740\n",
      "17      1        refugee     86    8.660624\n",
      "18      1     vulnerable     80    8.056395\n",
      "19      1          women     52    5.236657\n"
     ]
    }
   ],
   "source": [
    "# Group by 'label' and 'keyword', get counts\n",
    "counts = df.groupby(['label', 'keyword']).size().reset_index(name='Count')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(counts.head())\n",
    "\n",
    "# Get total rows per label\n",
    "label_totals = df['label'].value_counts()\n",
    "\n",
    "# Map the total count for each 'label' to the 'Count' DataFrame\n",
    "counts['Label_Total'] = counts['label'].map(label_totals)\n",
    "\n",
    "# Calculate percentage\n",
    "counts['Percentage'] = (counts['Count'] / counts['Label_Total']) * 100\n",
    "\n",
    "# Optional: Drop helper column if you don't need it\n",
    "counts.drop(columns=['Label_Total'], inplace=True)\n",
    "\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label country  Count\n",
      "0      0      au    504\n",
      "1      0      bd    468\n",
      "2      0      ca    484\n",
      "3      0      gb    483\n",
      "4      0      gh    448\n",
      "    label country  Count  Percentage\n",
      "0       0      au    504    5.318700\n",
      "1       0      bd    468    4.938793\n",
      "2       0      ca    484    5.107640\n",
      "3       0      gb    483    5.097087\n",
      "4       0      gh    448    4.727733\n",
      "5       0      hk    461    4.864922\n",
      "6       0      ie    485    5.118193\n",
      "7       0      in    491    5.181511\n",
      "8       0      jm    433    4.569439\n",
      "9       0      ke    494    5.213170\n",
      "10      0      lk    455    4.801604\n",
      "11      0      my    503    5.308147\n",
      "12      0      ng    465    4.907134\n",
      "13      0      nz    471    4.970452\n",
      "14      0      ph    478    5.044322\n",
      "15      0      pk    492    5.192064\n",
      "16      0      sg    497    5.244829\n",
      "17      0      tz    374    3.946813\n",
      "18      0      us    499    5.265935\n",
      "19      0      za    491    5.181511\n",
      "20      1      au     37    3.726083\n",
      "21      1      bd     44    4.431017\n",
      "22      1      ca     46    4.632427\n",
      "23      1      gb     57    5.740181\n",
      "24      1      gh     75    7.552870\n",
      "25      1      hk     29    2.920443\n",
      "26      1      ie     52    5.236657\n",
      "27      1      in     39    3.927492\n",
      "28      1      jm     57    5.740181\n",
      "29      1      ke     45    4.531722\n",
      "30      1      lk     49    4.934542\n",
      "31      1      my     43    4.330312\n",
      "32      1      ng     72    7.250755\n",
      "33      1      nz     47    4.733132\n",
      "34      1      ph     67    6.747231\n",
      "35      1      pk     53    5.337362\n",
      "36      1      sg     38    3.826788\n",
      "37      1      tz     41    4.128902\n",
      "38      1      us     44    4.431017\n",
      "39      1      za     58    5.840886\n"
     ]
    }
   ],
   "source": [
    "# Group by 'label' and 'keyword', get counts\n",
    "counts = df.groupby(['label', 'country']).size().reset_index(name='Count')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(counts.head())\n",
    "\n",
    "# Get total rows per label\n",
    "label_totals = df['label'].value_counts()\n",
    "\n",
    "# Map the total count for each 'label' to the 'Count' DataFrame\n",
    "counts['Label_Total'] = counts['label'].map(label_totals)\n",
    "\n",
    "# Calculate percentage\n",
    "counts['Percentage'] = (counts['Count'] / counts['Label_Total']) * 100\n",
    "\n",
    "# Optional: Drop helper column if you don't need it\n",
    "counts.drop(columns=['Label_Total'], inplace=True)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469\n"
     ]
    }
   ],
   "source": [
    "# Split in to dev set and test set\n",
    "train_ids = pd.read_csv('train_semeval_parids-labels.csv')\n",
    "dev_ids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "train_ids = train_ids.iloc[:, 0].astype(str).tolist()\n",
    "dev_ids = dev_ids.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "train_df = df[df['par_id'].isin(train_ids)]\n",
    "dev_df = df[df['par_id'].isin(dev_ids)]\n",
    "print(len(train_df)+len(dev_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = df['keyword'].unique().tolist()\n",
    "COUNTRIES = df['country'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, labels, keywords, countries, tokenizer, max_length=128):\n",
    "        # Tokenize texts\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "        # Compute extra features for each text (e.g. sentence length and keyword indicator)\n",
    "        #self.extra_features = [self.compute_extra_features(text) for text in texts]\n",
    "        self.extra_features = [self.extract_features(text, keyword, country) for text, keyword, country in zip(texts, keywords, countries)]\n",
    "        \n",
    "    def extract_features(self, text, keyword, country):\n",
    "        length = len(text.split())\n",
    "\n",
    "        # One-hot keyword presence\n",
    "        keyword_presence = [1 if keyword == kw else 0 for kw in KEYWORDS]\n",
    "\n",
    "        # One-hot encoding of country\n",
    "        country_encoding = [1 if country == c else 0 for c in COUNTRIES]\n",
    "\n",
    "        # Combine all features\n",
    "        features = [length] + keyword_presence + country_encoding\n",
    "        return features\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get tokenized text\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Get label\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        # Get extra features (convert to float tensor)\n",
    "        extra_feature = torch.tensor(self.extra_features[idx], dtype=torch.float)\n",
    "        item['extra_features'] = extra_feature\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, extra_feature_dim):\n",
    "        super(PCLModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size  # typically 768 for roberta-base\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Classifier takes concatenated [CLS] embedding and extra features\n",
    "        self.classifier = nn.Linear(hidden_size + extra_feature_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, extra_features, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output  # [CLS] token representation\n",
    "        # Concatenate the extra features\n",
    "        combined = torch.cat((cls_output, extra_features), dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.classifier(combined)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "#model_name = \"roberta-base\"  # You can switch to \"roberta-large\" if desired\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Load the tokenizer (if not already loaded)\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the custom model\n",
    "num_labels = 2  # binary classification\n",
    "extra_feature_dim = 1 + len(KEYWORDS) + len(COUNTRIES)\n",
    "model = PCLModel(model_name, num_labels, extra_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_texts = train_df[\"text\"].tolist()\n",
    "#train_labels = train_df[\"label\"].tolist()\n",
    "\n",
    "# Split train set into train and valiadation (90/10)\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "#    train_texts, train_labels, test_size=0.1, random_state=42\n",
    "#)\n",
    "\n",
    "# Create dataset objects for training and validation\n",
    "#train_dataset = PCLDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "#val_dataset = PCLDataset(val_texts, val_labels, tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 13968, Validation samples: 1553\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume train_df is your training DataFrame\n",
    "train_texts = train_df[\"text\"].tolist()\n",
    "train_labels = train_df[\"label\"].tolist()\n",
    "train_keywords = train_df[\"keyword\"].tolist()\n",
    "train_countries = train_df[\"country\"].tolist()\n",
    "\n",
    "# Separate majority and minority classes (assuming label 1 is minority)\n",
    "minority_indices = [i for i, label in enumerate(train_labels) if label == 1]\n",
    "minority_texts = [train_texts[i] for i in minority_indices]\n",
    "minority_keywords = [train_keywords[i] for i in minority_indices]\n",
    "minority_countries = [train_countries[i] for i in minority_indices]\n",
    "\n",
    "# Data augmentation with Contextual BERT substitution (no nltk needed here)\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased',\n",
    "    action=\"substitute\",  # or \"insert\"\n",
    "    device='cuda'  # or 'cpu'\n",
    ")\n",
    "\n",
    "augmented_texts = []\n",
    "augmented_keywords = []\n",
    "augmented_countries = []\n",
    "for text, keyword, country in zip(minority_texts, minority_keywords, minority_countries):\n",
    "    # Generate 8 augmentations per minority sample\n",
    "    augmented_texts.extend([aug.augment(text) for _ in range(9)])\n",
    "    augmented_keywords.extend([keyword] * 9)\n",
    "    augmented_countries.extend([country] * 9)\n",
    "\n",
    "augmented_labels = [1] * len(augmented_texts)\n",
    "\n",
    "# Combine augmented minority samples with the original data\n",
    "train_texts_balanced = train_texts + augmented_texts\n",
    "train_labels_balanced = train_labels + augmented_labels\n",
    "train_keywords_balanced = train_keywords + augmented_keywords\n",
    "train_countries_balanced = train_countries + augmented_countries\n",
    "\n",
    "# Ensure train_texts_balanced is a flat list (if augmentation returns nested list)\n",
    "train_texts_balanced = [text if isinstance(text, str) else text[0] for text in train_texts_balanced]\n",
    "\n",
    "# Split into training and validation sets (90/10)\n",
    "train_texts, val_texts, train_labels, val_labels, train_keywords, val_keywords, train_countries, val_countries = train_test_split(\n",
    "    train_texts_balanced, train_labels_balanced, train_keywords_balanced, train_countries_balanced, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataset objects (the extra features will be computed inside the dataset)\n",
    "train_dataset = PCLDataset(train_texts, train_labels, train_keywords, train_countries, tokenizer, max_length=128)\n",
    "val_dataset = PCLDataset(val_texts, val_labels, val_keywords, val_countries, tokenizer, max_length=128)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after augmentation:\n",
      "Class 0: 7581 instances\n",
      "Class 1: 7940 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution after augmentation:\")\n",
    "print(f\"Class 0: {train_labels_balanced.count(0)} instances\")\n",
    "print(f\"Class 1: {train_labels_balanced.count(1)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/eww24/nlpenv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # Output directory for model checkpoints\n",
    "    num_train_epochs=3,                # Number of training epochs\n",
    "    per_device_train_batch_size=16,     # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,     # Batch size for evaluation\n",
    "    warmup_steps=500,                  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                 # Strength of weight decay\n",
    "    logging_dir='./logs',              # Directory for storing logs\n",
    "    logging_steps=10,                  # Log every 10 steps\n",
    "    evaluation_strategy=\"steps\",       # Evaluate every 'eval_steps'\n",
    "    eval_steps=100,                    # Evaluate every 100 steps\n",
    "    save_steps=100,                    # Save checkpoint every 100 steps\n",
    "    load_best_model_at_end=True,       # Load the best model at the end\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,           # Lower eval_loss is better\n",
    "    disable_tqdm=False,\n",
    "    report_to=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='2619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/2619 05:10 < 28:52, 1.28 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.158826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.203832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.251350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.234541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.1339392829872668, metrics={'train_runtime': 311.2269, 'train_samples_per_second': 134.641, 'train_steps_per_second': 8.415, 'total_flos': 0.0, 'train_loss': 0.1339392829872668, 'epoch': 0.4581901489117984})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# Initialize and start the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if no improvement for 3 evaluations\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/98 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.15882599353790283, 'eval_runtime': 16.0089, 'eval_samples_per_second': 97.009, 'eval_steps_per_second': 6.122, 'epoch': 0.4581901489117984}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_pcl_model\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "save_directory = \"./fine_tuned_pcl_model\"\n",
    "trainer.save_model(save_directory)  # This saves the model to the specified directory\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./fine_tuned_pcl_model were not used when initializing RobertaForSequenceClassification: ['classifier.bias', 'classifier.weight', 'model.embeddings.LayerNorm.bias', 'model.embeddings.LayerNorm.weight', 'model.embeddings.position_embeddings.weight', 'model.embeddings.token_type_embeddings.weight', 'model.embeddings.word_embeddings.weight', 'model.encoder.layer.0.attention.output.LayerNorm.bias', 'model.encoder.layer.0.attention.output.LayerNorm.weight', 'model.encoder.layer.0.attention.output.dense.bias', 'model.encoder.layer.0.attention.output.dense.weight', 'model.encoder.layer.0.attention.self.key.bias', 'model.encoder.layer.0.attention.self.key.weight', 'model.encoder.layer.0.attention.self.query.bias', 'model.encoder.layer.0.attention.self.query.weight', 'model.encoder.layer.0.attention.self.value.bias', 'model.encoder.layer.0.attention.self.value.weight', 'model.encoder.layer.0.intermediate.dense.bias', 'model.encoder.layer.0.intermediate.dense.weight', 'model.encoder.layer.0.output.LayerNorm.bias', 'model.encoder.layer.0.output.LayerNorm.weight', 'model.encoder.layer.0.output.dense.bias', 'model.encoder.layer.0.output.dense.weight', 'model.encoder.layer.1.attention.output.LayerNorm.bias', 'model.encoder.layer.1.attention.output.LayerNorm.weight', 'model.encoder.layer.1.attention.output.dense.bias', 'model.encoder.layer.1.attention.output.dense.weight', 'model.encoder.layer.1.attention.self.key.bias', 'model.encoder.layer.1.attention.self.key.weight', 'model.encoder.layer.1.attention.self.query.bias', 'model.encoder.layer.1.attention.self.query.weight', 'model.encoder.layer.1.attention.self.value.bias', 'model.encoder.layer.1.attention.self.value.weight', 'model.encoder.layer.1.intermediate.dense.bias', 'model.encoder.layer.1.intermediate.dense.weight', 'model.encoder.layer.1.output.LayerNorm.bias', 'model.encoder.layer.1.output.LayerNorm.weight', 'model.encoder.layer.1.output.dense.bias', 'model.encoder.layer.1.output.dense.weight', 'model.encoder.layer.10.attention.output.LayerNorm.bias', 'model.encoder.layer.10.attention.output.LayerNorm.weight', 'model.encoder.layer.10.attention.output.dense.bias', 'model.encoder.layer.10.attention.output.dense.weight', 'model.encoder.layer.10.attention.self.key.bias', 'model.encoder.layer.10.attention.self.key.weight', 'model.encoder.layer.10.attention.self.query.bias', 'model.encoder.layer.10.attention.self.query.weight', 'model.encoder.layer.10.attention.self.value.bias', 'model.encoder.layer.10.attention.self.value.weight', 'model.encoder.layer.10.intermediate.dense.bias', 'model.encoder.layer.10.intermediate.dense.weight', 'model.encoder.layer.10.output.LayerNorm.bias', 'model.encoder.layer.10.output.LayerNorm.weight', 'model.encoder.layer.10.output.dense.bias', 'model.encoder.layer.10.output.dense.weight', 'model.encoder.layer.11.attention.output.LayerNorm.bias', 'model.encoder.layer.11.attention.output.LayerNorm.weight', 'model.encoder.layer.11.attention.output.dense.bias', 'model.encoder.layer.11.attention.output.dense.weight', 'model.encoder.layer.11.attention.self.key.bias', 'model.encoder.layer.11.attention.self.key.weight', 'model.encoder.layer.11.attention.self.query.bias', 'model.encoder.layer.11.attention.self.query.weight', 'model.encoder.layer.11.attention.self.value.bias', 'model.encoder.layer.11.attention.self.value.weight', 'model.encoder.layer.11.intermediate.dense.bias', 'model.encoder.layer.11.intermediate.dense.weight', 'model.encoder.layer.11.output.LayerNorm.bias', 'model.encoder.layer.11.output.LayerNorm.weight', 'model.encoder.layer.11.output.dense.bias', 'model.encoder.layer.11.output.dense.weight', 'model.encoder.layer.2.attention.output.LayerNorm.bias', 'model.encoder.layer.2.attention.output.LayerNorm.weight', 'model.encoder.layer.2.attention.output.dense.bias', 'model.encoder.layer.2.attention.output.dense.weight', 'model.encoder.layer.2.attention.self.key.bias', 'model.encoder.layer.2.attention.self.key.weight', 'model.encoder.layer.2.attention.self.query.bias', 'model.encoder.layer.2.attention.self.query.weight', 'model.encoder.layer.2.attention.self.value.bias', 'model.encoder.layer.2.attention.self.value.weight', 'model.encoder.layer.2.intermediate.dense.bias', 'model.encoder.layer.2.intermediate.dense.weight', 'model.encoder.layer.2.output.LayerNorm.bias', 'model.encoder.layer.2.output.LayerNorm.weight', 'model.encoder.layer.2.output.dense.bias', 'model.encoder.layer.2.output.dense.weight', 'model.encoder.layer.3.attention.output.LayerNorm.bias', 'model.encoder.layer.3.attention.output.LayerNorm.weight', 'model.encoder.layer.3.attention.output.dense.bias', 'model.encoder.layer.3.attention.output.dense.weight', 'model.encoder.layer.3.attention.self.key.bias', 'model.encoder.layer.3.attention.self.key.weight', 'model.encoder.layer.3.attention.self.query.bias', 'model.encoder.layer.3.attention.self.query.weight', 'model.encoder.layer.3.attention.self.value.bias', 'model.encoder.layer.3.attention.self.value.weight', 'model.encoder.layer.3.intermediate.dense.bias', 'model.encoder.layer.3.intermediate.dense.weight', 'model.encoder.layer.3.output.LayerNorm.bias', 'model.encoder.layer.3.output.LayerNorm.weight', 'model.encoder.layer.3.output.dense.bias', 'model.encoder.layer.3.output.dense.weight', 'model.encoder.layer.4.attention.output.LayerNorm.bias', 'model.encoder.layer.4.attention.output.LayerNorm.weight', 'model.encoder.layer.4.attention.output.dense.bias', 'model.encoder.layer.4.attention.output.dense.weight', 'model.encoder.layer.4.attention.self.key.bias', 'model.encoder.layer.4.attention.self.key.weight', 'model.encoder.layer.4.attention.self.query.bias', 'model.encoder.layer.4.attention.self.query.weight', 'model.encoder.layer.4.attention.self.value.bias', 'model.encoder.layer.4.attention.self.value.weight', 'model.encoder.layer.4.intermediate.dense.bias', 'model.encoder.layer.4.intermediate.dense.weight', 'model.encoder.layer.4.output.LayerNorm.bias', 'model.encoder.layer.4.output.LayerNorm.weight', 'model.encoder.layer.4.output.dense.bias', 'model.encoder.layer.4.output.dense.weight', 'model.encoder.layer.5.attention.output.LayerNorm.bias', 'model.encoder.layer.5.attention.output.LayerNorm.weight', 'model.encoder.layer.5.attention.output.dense.bias', 'model.encoder.layer.5.attention.output.dense.weight', 'model.encoder.layer.5.attention.self.key.bias', 'model.encoder.layer.5.attention.self.key.weight', 'model.encoder.layer.5.attention.self.query.bias', 'model.encoder.layer.5.attention.self.query.weight', 'model.encoder.layer.5.attention.self.value.bias', 'model.encoder.layer.5.attention.self.value.weight', 'model.encoder.layer.5.intermediate.dense.bias', 'model.encoder.layer.5.intermediate.dense.weight', 'model.encoder.layer.5.output.LayerNorm.bias', 'model.encoder.layer.5.output.LayerNorm.weight', 'model.encoder.layer.5.output.dense.bias', 'model.encoder.layer.5.output.dense.weight', 'model.encoder.layer.6.attention.output.LayerNorm.bias', 'model.encoder.layer.6.attention.output.LayerNorm.weight', 'model.encoder.layer.6.attention.output.dense.bias', 'model.encoder.layer.6.attention.output.dense.weight', 'model.encoder.layer.6.attention.self.key.bias', 'model.encoder.layer.6.attention.self.key.weight', 'model.encoder.layer.6.attention.self.query.bias', 'model.encoder.layer.6.attention.self.query.weight', 'model.encoder.layer.6.attention.self.value.bias', 'model.encoder.layer.6.attention.self.value.weight', 'model.encoder.layer.6.intermediate.dense.bias', 'model.encoder.layer.6.intermediate.dense.weight', 'model.encoder.layer.6.output.LayerNorm.bias', 'model.encoder.layer.6.output.LayerNorm.weight', 'model.encoder.layer.6.output.dense.bias', 'model.encoder.layer.6.output.dense.weight', 'model.encoder.layer.7.attention.output.LayerNorm.bias', 'model.encoder.layer.7.attention.output.LayerNorm.weight', 'model.encoder.layer.7.attention.output.dense.bias', 'model.encoder.layer.7.attention.output.dense.weight', 'model.encoder.layer.7.attention.self.key.bias', 'model.encoder.layer.7.attention.self.key.weight', 'model.encoder.layer.7.attention.self.query.bias', 'model.encoder.layer.7.attention.self.query.weight', 'model.encoder.layer.7.attention.self.value.bias', 'model.encoder.layer.7.attention.self.value.weight', 'model.encoder.layer.7.intermediate.dense.bias', 'model.encoder.layer.7.intermediate.dense.weight', 'model.encoder.layer.7.output.LayerNorm.bias', 'model.encoder.layer.7.output.LayerNorm.weight', 'model.encoder.layer.7.output.dense.bias', 'model.encoder.layer.7.output.dense.weight', 'model.encoder.layer.8.attention.output.LayerNorm.bias', 'model.encoder.layer.8.attention.output.LayerNorm.weight', 'model.encoder.layer.8.attention.output.dense.bias', 'model.encoder.layer.8.attention.output.dense.weight', 'model.encoder.layer.8.attention.self.key.bias', 'model.encoder.layer.8.attention.self.key.weight', 'model.encoder.layer.8.attention.self.query.bias', 'model.encoder.layer.8.attention.self.query.weight', 'model.encoder.layer.8.attention.self.value.bias', 'model.encoder.layer.8.attention.self.value.weight', 'model.encoder.layer.8.intermediate.dense.bias', 'model.encoder.layer.8.intermediate.dense.weight', 'model.encoder.layer.8.output.LayerNorm.bias', 'model.encoder.layer.8.output.LayerNorm.weight', 'model.encoder.layer.8.output.dense.bias', 'model.encoder.layer.8.output.dense.weight', 'model.encoder.layer.9.attention.output.LayerNorm.bias', 'model.encoder.layer.9.attention.output.LayerNorm.weight', 'model.encoder.layer.9.attention.output.dense.bias', 'model.encoder.layer.9.attention.output.dense.weight', 'model.encoder.layer.9.attention.self.key.bias', 'model.encoder.layer.9.attention.self.key.weight', 'model.encoder.layer.9.attention.self.query.bias', 'model.encoder.layer.9.attention.self.query.weight', 'model.encoder.layer.9.attention.self.value.bias', 'model.encoder.layer.9.attention.self.value.weight', 'model.encoder.layer.9.intermediate.dense.bias', 'model.encoder.layer.9.intermediate.dense.weight', 'model.encoder.layer.9.output.LayerNorm.bias', 'model.encoder.layer.9.output.LayerNorm.weight', 'model.encoder.layer.9.output.dense.bias', 'model.encoder.layer.9.output.dense.weight', 'model.pooler.dense.bias', 'model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./fine_tuned_pcl_model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832\n"
     ]
    }
   ],
   "source": [
    "# Load the final test set\n",
    "rows=[]\n",
    "with open(os.path.join('task4_test.tsv')) as f:\n",
    "    for line in f.readlines():\n",
    "        par_id=line.strip().split('\\t')[0]\n",
    "        art_id = line.strip().split('\\t')[1]\n",
    "        keyword=line.strip().split('\\t')[2]\n",
    "        country=line.strip().split('\\t')[3]\n",
    "        rows.append(\n",
    "            {'par_id':par_id,\n",
    "            'art_id':art_id,\n",
    "            'keyword':keyword,\n",
    "            'country':country,\n",
    "            'text':t, \n",
    "            }\n",
    "            )\n",
    "test_df=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text']) \n",
    "print(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df[\"text\"].tolist()\n",
    "#test_dataset = PCLDataset(test_texts, tokenizer, max_length=128)\n",
    "\n",
    "dev_texts = dev_df[\"text\"].tolist()\n",
    "dev_labels = dev_df[\"label\"].tolist()\n",
    "dev_keywords = dev_df[\"keyword\"].tolist()\n",
    "dev_countries = dev_df[\"country\"].tolist()\n",
    "\n",
    "\n",
    "dev_dataset = PCLDataset(dev_texts, dev_labels, dev_keywords, dev_countries, tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Accuracy: 0.9083094555873925\n",
      "Dev F1: 0.12727272727272726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Get predictions using the trainer (or custom loop)\n",
    "pred_output = trainer.predict(dev_dataset)\n",
    "pred_labels = np.argmax(pred_output.predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(dev_labels, pred_labels)\n",
    "f1 = f1_score(dev_labels, pred_labels, average='binary')\n",
    "print(\"Dev Accuracy:\", accuracy)\n",
    "print(\"Dev F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1897\n",
      "Accuracy: 0.9059216809933143 F1: 0.029556650246305417\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded model to make predictions\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "loaded_model.eval()\n",
    "correct = 0\n",
    "predictions = []\n",
    "for i in range(len(dev_texts)):\n",
    "    text = dev_texts[i]\n",
    "    encodings = loaded_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    loaded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**encodings)\n",
    "        pred_label = torch.argmax(outputs.logits, dim=1)\n",
    "        if pred_label.tolist()[0] == dev_labels[i]:\n",
    "            correct += 1\n",
    "        #print(\"Predictions:\", predictions.tolist()[0], \"Actual label:\", dev_labels[i])\n",
    "        predictions.append(pred_label.tolist()[0])\n",
    "print(correct)\n",
    "accuracy = correct/len(dev_texts)\n",
    "f1 = f1_score(dev_labels, predictions, average='binary')\n",
    "print(\"Accuracy:\", accuracy, \"F1:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
