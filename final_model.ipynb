{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469\n",
      "9476\n",
      "993\n"
     ]
    }
   ],
   "source": [
    "# Load data to panda data frame\n",
    "rows=[]\n",
    "with open(os.path.join('dontpatronizeme_pcl.tsv')) as f:\n",
    "    for line in f.readlines()[4:]:\n",
    "        par_id=line.strip().split('\\t')[0]\n",
    "        art_id = line.strip().split('\\t')[1]\n",
    "        keyword=line.strip().split('\\t')[2]\n",
    "        country=line.strip().split('\\t')[3]\n",
    "        t=line.strip().split('\\t')[4]#.lower()\n",
    "        l=line.strip().split('\\t')[-1]\n",
    "        if l=='0' or l=='1':\n",
    "            lbin=0\n",
    "        else:\n",
    "            lbin=1\n",
    "        rows.append(\n",
    "            {'par_id':par_id,\n",
    "            'art_id':art_id,\n",
    "            'keyword':keyword,\n",
    "            'country':country,\n",
    "            'text':t, \n",
    "            'label':lbin, \n",
    "            'orig_label':l\n",
    "            }\n",
    "            )\n",
    "df=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text', 'label', 'orig_label']) \n",
    "print(df.shape[0])\n",
    "print(df[df['label'] == 0].shape[0])\n",
    "print(df[df['label'] == 1].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.87779653862389\n",
      "53.620342396777446\n",
      "48.42248543318369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2999732/4254412181.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noPCL['text_Length'] = noPCL['text'].apply(lambda x: len(x.split()))\n",
      "/tmp/ipykernel_2999732/4254412181.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PCL['text_Length'] = PCL['text'].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter rows with a specific label (e.g., 'negative')\n",
    "noPCL = df[df['label'] == 0]\n",
    "PCL = df[df['label'] == 1]\n",
    "average = df\n",
    "\n",
    "# Get sentence length (in words)\n",
    "noPCL['text_Length'] = noPCL['text'].apply(lambda x: len(x.split()))\n",
    "PCL['text_Length'] = PCL['text'].apply(lambda x: len(x.split()))\n",
    "average['text_length']  = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate average length\n",
    "average_length_noPCL = noPCL['text_Length'].mean()\n",
    "average_length_PCL = PCL['text_Length'].mean()\n",
    "average_l = average['text_length'].mean()\n",
    "\n",
    "print(average_length_noPCL)\n",
    "print(average_length_PCL)\n",
    "print(average_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919.4444444444445\n",
      "701.9259259259259\n"
     ]
    }
   ],
   "source": [
    "# Get unique values from a column\n",
    "total = df[df['keyword']=='vulnerable'].shape[0]\n",
    "unique_keywords_noPCL = noPCL[noPCL['keyword']=='vulnerable'].shape[0]\n",
    "unique_keywords_PCL = PCL[PCL['keyword']=='vulnerable'].shape[0]\n",
    "\n",
    "print((unique_keywords_noPCL/total)*(993))\n",
    "print((unique_keywords_PCL/total)*(9476))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469\n"
     ]
    }
   ],
   "source": [
    "# Split in to dev set and test set\n",
    "train_ids = pd.read_csv('train_semeval_parids-labels.csv')\n",
    "dev_ids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "train_ids = train_ids.iloc[:, 0].astype(str).tolist()\n",
    "dev_ids = dev_ids.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "train_df = df[df['par_id'].isin(train_ids)]\n",
    "dev_df = df[df['par_id'].isin(dev_ids)]\n",
    "print(len(train_df)+len(dev_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get tokenized text\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Get label\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        # Suppose extra_features is a precomputed list of feature vectors:\n",
    "        extra_feature = torch.tensor(self.extra_features[idx])\n",
    "        item['extra_features'] = extra_feature\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "model_name = \"roberta-base\"  # You can switch to \"roberta-large\" if desired\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_texts = train_df[\"text\"].tolist()\n",
    "#train_labels = train_df[\"label\"].tolist()\n",
    "\n",
    "# Split train set into train and valiadation (90/10)\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "#    train_texts, train_labels, test_size=0.1, random_state=42\n",
    "#)\n",
    "\n",
    "# Create dataset objects for training and validation\n",
    "#train_dataset = PCLDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "#val_dataset = PCLDataset(val_texts, val_labels, tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 13254, Validation samples: 1473\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts = train_df[\"text\"].tolist()\n",
    "train_labels = train_df[\"label\"].tolist()\n",
    "\n",
    "# Separate majority and minority classes\n",
    "minority_texts = [train_texts[i] for i in range(len(train_labels)) if train_labels[i] == 1]\n",
    "minority_labels = [1] * len(minority_texts)\n",
    "\n",
    "# Data augmentation with Contextual BERT substitution\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased',\n",
    "    action=\"substitute\",  # You can also use \"insert\"\n",
    "    device='cuda'  # or 'cpu'\n",
    ")\n",
    "\n",
    "augmented_texts = []\n",
    "for text in minority_texts:\n",
    "    augmented_texts.extend([aug.augment(text) for _ in range(8)])  # 8 augmentations per sample\n",
    "\n",
    "#augmented_texts = [aug.augment(text) for text in minority_texts]\n",
    "augmented_labels = [1] * len(augmented_texts)\n",
    "\n",
    "# Combine augmented minority samples with the original data\n",
    "train_texts_balanced = train_texts + augmented_texts\n",
    "train_labels_balanced = train_labels + augmented_labels\n",
    "\n",
    "# Ensure train_texts_balanced is a flat list, not nested\n",
    "train_texts_balanced = [text if isinstance(text, str) else text[0] for text in train_texts_balanced]\n",
    "\n",
    "\n",
    "# Split into training and validation sets (90/10)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts_balanced, train_labels_balanced, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset objects for training and validation\n",
    "train_dataset = PCLDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = PCLDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after augmentation:\n",
      "Class 0: 7581 instances\n",
      "Class 1: 7146 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution after augmentation:\")\n",
    "print(f\"Class 0: {train_labels_balanced.count(0)} instances\")\n",
    "print(f\"Class 1: {train_labels_balanced.count(1)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/eww24/nlpenv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # Output directory for model checkpoints\n",
    "    num_train_epochs=3,                # Number of training epochs\n",
    "    per_device_train_batch_size=8,     # Batch size per device during training\n",
    "    per_device_eval_batch_size=16,     # Batch size for evaluation\n",
    "    warmup_steps=500,                  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                 # Strength of weight decay\n",
    "    logging_dir='./logs',              # Directory for storing logs\n",
    "    logging_steps=10,                  # Log every 10 steps\n",
    "    evaluation_strategy=\"steps\",       # Evaluate every 'eval_steps'\n",
    "    eval_steps=100,                    # Evaluate every 100 steps\n",
    "    save_steps=100,                    # Save checkpoint every 100 steps\n",
    "    load_best_model_at_end=True,       # Load the best model at the end\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    disable_tqdm=False,\n",
    "    report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4971' max='4971' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4971/4971 44:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.225592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.333111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>0.190876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.203376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.213682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.241924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.245704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.189348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.277011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.219028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.217759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.249933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.208175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.215527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.218480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.215564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.139800</td>\n",
       "      <td>0.216422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.233115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>0.202981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.209184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.259390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.220777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.199307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.206255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.220231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.221340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.201433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>0.195170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.217865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.181600</td>\n",
       "      <td>0.221511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.223571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.209038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.202583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.200062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.206253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.198783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.190824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.227453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.206741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.200416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.210975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.230500</td>\n",
       "      <td>0.196858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.169400</td>\n",
       "      <td>0.205140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.206268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.220442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.226115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4971, training_loss=0.21338168493530305, metrics={'train_runtime': 2650.9833, 'train_samples_per_second': 14.999, 'train_steps_per_second': 1.875, 'total_flos': 2615455445806080.0, 'train_loss': 0.21338168493530305, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and start the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.18934808671474457, 'eval_runtime': 15.1699, 'eval_samples_per_second': 97.1, 'eval_steps_per_second': 6.131, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_pcl_model\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "save_directory = \"./fine_tuned_pcl_model\"\n",
    "trainer.save_model(save_directory)  # This saves the model to the specified directory\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832\n"
     ]
    }
   ],
   "source": [
    "# Load the final test set\n",
    "rows=[]\n",
    "with open(os.path.join('task4_test.tsv')) as f:\n",
    "    for line in f.readlines():\n",
    "        par_id=line.strip().split('\\t')[0]\n",
    "        art_id = line.strip().split('\\t')[1]\n",
    "        keyword=line.strip().split('\\t')[2]\n",
    "        country=line.strip().split('\\t')[3]\n",
    "        rows.append(\n",
    "            {'par_id':par_id,\n",
    "            'art_id':art_id,\n",
    "            'keyword':keyword,\n",
    "            'country':country,\n",
    "            'text':t, \n",
    "            }\n",
    "            )\n",
    "test_df=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text']) \n",
    "print(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df[\"text\"].tolist()\n",
    "#test_dataset = PCLDataset(test_texts, tokenizer, max_length=128)\n",
    "\n",
    "dev_texts = dev_df[\"text\"].tolist()\n",
    "dev_labels = dev_df[\"label\"].tolist()\n",
    "\n",
    "dev_dataset = PCLDataset(dev_texts, dev_labels, tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1897\n",
      "Accuracy: 0.9059216809933143 F1: 0.029556650246305417\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded model to make predictions\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "loaded_model.eval()\n",
    "correct = 0\n",
    "predictions = []\n",
    "for i in range(len(dev_texts)):\n",
    "    text = dev_texts[i]\n",
    "    encodings = loaded_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    loaded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**encodings)\n",
    "        pred_label = torch.argmax(outputs.logits, dim=1)\n",
    "        if pred_label.tolist()[0] == dev_labels[i]:\n",
    "            correct += 1\n",
    "        #print(\"Predictions:\", predictions.tolist()[0], \"Actual label:\", dev_labels[i])\n",
    "        predictions.append(pred_label.tolist()[0])\n",
    "print(correct)\n",
    "accuracy = correct/len(dev_texts)\n",
    "f1 = f1_score(dev_labels, predictions, average='binary')\n",
    "print(\"Accuracy:\", accuracy, \"F1:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
